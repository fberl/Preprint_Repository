from analysis_scripts.logistic_regression import logit, logistic_regression_no_bias
import numpy as np
import matplotlib.pyplot as plt
import torch
import matplotlib.gridspec as gridspec
import matplotlib
import os
from scipy.optimize import minimize
matplotlib.rcParams.update({'font.size': 6})

def generate_data(model,env,nits=1):
    """
    Given a model and an environment, this function will generate firing rates for episodes. 
    """
    episode_actions = []
    episode_rewards = []
    episode_states = []
    episode_hiddens = []
    
    for eps in range(nits):
        state = env.reset()
        done = False
        rewards = [] 
        actions = []
        states = []
        hiddens = []
        hidden = None
        last_action = env.action_space.sample()
        last_action = torch.nn.functional.one_hot(torch.tensor(last_action),num_classes=2).numpy()
        while not done:
            action, _, hidden = model.policy.get_action(state, last_action, rewards,hidden)
            state, reward, done, _ = env.step(action[1])
            rewards.append(reward)
            actions.append(action.detach().squeeze().numpy()[1])
            states.append(state[1])
            hiddens.append(hidden.squeeze().view(-1,1))
            last_action = action
        
        episode_actions.append(np.vstack(actions))
        episode_rewards.append(np.vstack(rewards))
        episode_states.append(np.vstack(states))
        episode_hiddens.append(torch.hstack(hiddens))
    
    episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
        
    return episode_states, episode_actions, episode_rewards, episode_hiddens

def format_data(episode_states, episode_actions, episode_rewards, episode_hiddens):
    """
    Given the data generated by generate_data, this function will format the data into tensors. 
    """
    episode_states = np.stack(episode_states).squeeze()
    episode_actions = np.stack(episode_actions).squeeze()
    episode_rewards = np.stack(episode_rewards).squeeze()
    episode_hiddens = torch.stack(episode_hiddens)
    
    return episode_states, episode_actions, episode_rewards, episode_hiddens
    

def ravel_order(mat,order):
    """Aligns the data so that the data at time t is used to fit the regressor at time t-order.

    Args:
        mat (_type_): _description_
        order (_type_): _description_

    Returns:
        _type_: _description_
    """
    if order >= 1:
        return mat[:,:-order].squeeze().ravel()
    else:
        return mat.squeeze().ravel()
    # return mat[:,order:].ravel()

def ravel_hidden(hidden,order,hidden_dim):
    # starting shape: [nits, time, hidden_size]
    # ending shape: [nits*(time-order), hidden_size]
    hidden = torch.permute(hidden, (1,0,2))
    hidden = hidden[:,:,order:].reshape(hidden_dim, -1) #make sure this reshaped proper order
    
    return hidden.detach().numpy().T

def plot_population_decoding(model, env, order = 1, nits = 1, data = False):
    """
    Given a model and an environment, this function will generate firing rates for a nits episodes and then use that to fit a regressor. 
    
    For example, data at time t is used to fit the regressor at time t-order, in order to check coding of that variable at that order
    """
    if data:
        episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
        episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
    else:
        episode_states, episode_actions, episode_rewards, episode_hiddens = generate_data(model, env, nits)
        
    targets = [episode_actions, episode_rewards]
    all_vars = {}
    all_accuracy = {}
    label = ['action','reward']
    for i,tar in enumerate(targets):
        all_vars[label[i]] = np.zeros((model.hidden_dim,order))
        all_accuracy[label[i]] = []
        for ord in range(order):
            Y = ravel_order(tar,ord)
            #Apply ReLU to hidden state to get firing rate:
            episode_hiddens = model.activation(episode_hiddens)
            X = ravel_hidden(episode_hiddens,ord, model.hidden_dim)
            fit = logistic_regression_no_bias(X,Y).x
           
            # 0 is left / unrewarded, 1 is right / rewarded
            accuracy = np.mean(np.round(logit(X,fit,0)) == Y)
            all_accuracy[label[i]].append(accuracy)
            # check variance explained by each neuron (column in X)
            vars = pseudo_variance_explained(X,Y,fit)    
            all_vars[label[i]][:,ord] = vars[0]        
            # all_vars[label[i]][:,ord] = vars[1]        
        
    if data:
        return all_accuracy, all_vars
    
    plot_figure(all_accuracy, all_vars, model.hidden_dim, order)
        
    
def plot_figure(accuracy,vars, num_neurons, order):
    fig, axs = create_figure(num_neurons, order)
    for key in accuracy.keys():
        axs[0].plot(accuracy[key], label = key)
        axs[0].set_title('Population Decoding Predictive Accuracy')
        axs[0].set_xlabel('order')
        axs[0].set_ylabel('accuracy')
        axs[0].axhline(0.5,linestyle='dashed', alpha=0.5)
        axs[0].legend()
    
    for i,key in enumerate(['action','reward']):
        im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        axs[i+1].set_title(key + ' coding')
        axs[i+1].set_xlabel('order')
        axs[i+1].set_ylabel('neuron')
    
    fig.colorbar(im, cax=axs[3])

        
def create_figure(order, num_neurons):
    """
    Creates a figure with subplots for each order of the regressor. 
    Width varies with order so we can keep a the visualizations clean
    """

    fig = plt.figure(constrained_layout=True, figsize=(.2*order, 4+.2*num_neurons))
    spec = gridspec.GridSpec(ncols=11, nrows=2, figure=fig)
    ax1 = fig.add_subplot(spec[0,:])
    ax2 = fig.add_subplot(spec[1,:5])
    ax3 = fig.add_subplot(spec[1,5:10])
    cbar = fig.add_subplot(spec[1,10])
    
    
    axs = [ax1, ax2, ax3, cbar]
    return fig, axs

def logistic_rsq(X,fit):
    """
    Computes the pseudo R^2 value for a logistic regression. 
    """
    ps = logit(X,fit,0)
    return np.var(ps) / (np.var(ps) + np.sum(ps*(1-ps)/len(ps)))

def pseudo_variance_explained(X, Y, fit):
    """
    Computes variance explained for each neuron in the population for target Y
    Since this is a logistic regression, we can't use the same metric as in linear regression
    Instead, we fit each regressor individually and see how it performs as a predictor for the target,
    and compare it to the total 

    Args:
        X (_type_): _description_
        Y (_type_): _description_
        fit (_type_): _description_
    """
    accs = [] # accuracy ratio of each neuron to the population. baseline accuracy is 50% so we must subtract that from both
    Rsqs = [] #rsq0 given by var(P) / (var(P) + Sum(P(1-P)/n))
    
    fit_accuracy = np.mean(np.round(logit(X,fit,0)) == Y)
    
    for i in range(X.shape[1]):
        subfit = logistic_regression_no_bias(X[:,i],Y).x
        subfit_accuracy = np.mean(np.round(logit(X[:,i],subfit,0)) == Y)
        # ratio = (subfit_accuracy - 0.5) / (fit_accuracy - 0.5+1e-4)
        ratio = subfit_accuracy
        accs.append(ratio)
        
        Rsq = logistic_rsq(X[i,:],subfit)
        Rsqs.append(Rsq)
    
    #normalize columns of the ratio matrix
    accs = np.array(accs)
    accs = accs / np.sum(accs)
    
    
    return accs, Rsqs
    

def linear_regression(X,Y):
    """
    Fits a linear regression model to the data. 
    """
    return np.linalg.lstsq(X,Y,rcond=None)

    # return minimize(objective_function_linear_l2, np.zeros(X.shape[0]), args=(X,Y), method='BFGS').x

def linear_regression_l2(X,Y):
    """
    Fits a linear regression model to the data. 
    """
    return minimize(objective_function_linear_l2, np.zeros(X.shape[0]), args=(X,Y),tol=1e-9)

def linear_regression_bias(X,Y):
    """
    Fits a linear regression model to the data. 
    """
    #add column to X
    X = np.vstack([X,np.ones(X.shape[0])]).T# is this right?
    return np.linalg.lstsq(X,Y,rcond=None)

    # return minimize(objective_function_linear_l2, np.zeros(X.shape[0]), args=(X,Y), method='BFGS').x


def objective_function_linear_l2(beta, X, Y):
    return np.sum((Y - np.dot(X.T,beta))**2) + 0.05*np.sum(beta**2) + 0*np.sum(np.abs(beta))

def yhat(X,beta):
    return np.dot(X.T,beta)

def RSS(Y,Yhat):
    return np.sum((Y - Yhat)**2)

def stack_order(mat,order):
    '''
    Stacks a matrix so that the data from time t-order to t is used to fit the firing rate
    '''
    mat_stack = []
    for i in range(order):
        if mat.ndim > 1:
            if i == 0:
                mat_stack.append(mat[:,order:])
            else:
                mat_stack.append(mat[:,order-i:-i])
        else:
            if i == 0:
                mat_stack.append(mat[order:])
            else:
                mat_stack.append(mat[order-i:-i])
    return np.stack(mat_stack).reshape(order,-1)

def stack_hidden(hidden, order, hidden_size):
    # starting shape: [nits, time, hidden_size]
    # ending shape: [nits*(time-order), hidden_size]
    hidden = torch.permute(hidden, (1,0,2))
    hidden = hidden[:,:,order:].reshape(hidden_size, -1) #make sure this reshaped proper order
    
    return hidden.detach().numpy()
    
def variance_explained(Y,actions, rewards, combined,order):
    '''
    given a fit, returns the variance explained by the model as well as by each individual regressor 
    '''
    
    
    delta_action_rsqs = []
    delta_reward_rsqs = []
    delta_combined_rsqs = []
    
    Y_rec = np.maximum(0,Y)
    TSS = np.var(Y_rec) * len(Y_rec)
    
    def RSS_rec(X,fit,Y):
        return np.sum((Y - np.maximum(0,np.dot(X.T,fit)))**2)
    
    delta_a = delta_r = delta_c = 0
    
    for i in range(order):

    #     action_fit = linear_regression(actions[:i+1].T,Y)
    #     reward_fit = linear_regression(rewards[:i+1].T,Y)
    #     combined_fit = linear_regression(combined[:i+1].T,Y)
    #     delta_a = np.nan_to_num(1 - RSS(Y,yhat(actions[:i+1],action_fit)) / TSS - sum(delta_action_rsqs) ,0)
    #     delta_r = np.nan_to_num(1 - RSS(Y,yhat(rewards[:i+1],reward_fit)) / TSS - sum(delta_reward_rsqs), 0) 
    #     delta_c = np.nan_to_num(1 - RSS(Y,yhat(combined[:i+1],combined_fit)) / TSS - sum(delta_combined_rsqs), 0)
        
        action_fit = linear_regression(actions[:i+1].T,Y_rec)
        reward_fit = linear_regression(rewards[:i+1].T,Y_rec)
        combined_fit = linear_regression(combined[:i+1].T,Y_rec)
        
        delta_a = np.maximum(np.nan_to_num(1 - RSS_rec(actions[:i+1],action_fit[0],Y_rec) / TSS - sum(delta_action_rsqs), 0),0)
        delta_r = np.maximum(np.nan_to_num(1 - RSS_rec(rewards[:i+1],reward_fit[0],Y_rec) / TSS - sum(delta_reward_rsqs), 0),0)
        delta_c = np.maximum(np.nan_to_num(1 - RSS_rec(combined[:i+1],combined_fit[0],Y_rec) / TSS - sum(delta_combined_rsqs), 0),0)        
        
        # delta_a = np.fmax(0,np.nan_to_num(1 - action_fit[1] / TSS - sum(delta_action_rsqs), 0))
        # delta_r = np.fmax(0,np.nan_to_num(1 - reward_fit[1] / TSS - sum(delta_reward_rsqs), 0))
        # delta_c = np.fmax(0,np.nan_to_num(1 - combined_fit[1] / TSS - sum(delta_combined_rsqs), 0))
            
        delta_action_rsqs.append(delta_a)
        delta_reward_rsqs.append(delta_r)
        delta_combined_rsqs.append(delta_c) 
    
    return delta_action_rsqs, delta_reward_rsqs, delta_combined_rsqs


def variance_explained_hidden(Y,actions, rewards, combined,order):
    '''
    given a fit, returns the variance explained by the model as well as by each individual regressor 
    '''
    
    
    delta_action_rsqs = []
    delta_reward_rsqs = []
    delta_combined_rsqs = []
    
    # Y_rec = np.maximum(0,Y)
    TSS = np.var(Y) * len(Y)
    
    def RSS_rec(X,fit,Y):
        return np.sum((Y - np.dot(X.T,fit))**2)
    
    delta_a = delta_r = delta_c = 0
    
    for i in range(order):

    #     action_fit = linear_regression(actions[:i+1].T,Y)
    #     reward_fit = linear_regression(rewards[:i+1].T,Y)
    #     combined_fit = linear_regression(combined[:i+1].T,Y)
    #     delta_a = np.nan_to_num(1 - RSS(Y,yhat(actions[:i+1],action_fit)) / TSS - sum(delta_action_rsqs) ,0)
    #     delta_r = np.nan_to_num(1 - RSS(Y,yhat(rewards[:i+1],reward_fit)) / TSS - sum(delta_reward_rsqs), 0) 
    #     delta_c = np.nan_to_num(1 - RSS(Y,yhat(combined[:i+1],combined_fit)) / TSS - sum(delta_combined_rsqs), 0)
        
        action_fit = linear_regression(actions[:i+1].T,Y)
        reward_fit = linear_regression(rewards[:i+1].T,Y)
        combined_fit = linear_regression(combined[:i+1].T,Y)
        
        delta_a = np.maximum(np.nan_to_num(1 - RSS_rec(actions[:i+1],action_fit[0],Y) / TSS - sum(delta_action_rsqs), 0),0)
        delta_r = np.maximum(np.nan_to_num(1 - RSS_rec(rewards[:i+1],reward_fit[0],Y) / TSS - sum(delta_reward_rsqs), 0),0)
        delta_c = np.maximum(np.nan_to_num(1 - RSS_rec(combined[:i+1],combined_fit[0],Y) / TSS - sum(delta_combined_rsqs), 0),0)        
        
        # delta_a = np.fmax(0,np.nan_to_num(1 - action_fit[1] / TSS - sum(delta_action_rsqs), 0))
        # delta_r = np.fmax(0,np.nan_to_num(1 - reward_fit[1] / TSS - sum(delta_reward_rsqs), 0))
        # delta_c = np.fmax(0,np.nan_to_num(1 - combined_fit[1] / TSS - sum(delta_combined_rsqs), 0))
            
        delta_action_rsqs.append(delta_a)
        delta_reward_rsqs.append(delta_r)
        delta_combined_rsqs.append(delta_c) 
    
    return delta_action_rsqs, delta_reward_rsqs, delta_combined_rsqs

def plot_population_encoding(model, env, order = 1, nits = 1, data=False):
    '''
    Uses behavioral data to try to predict individual firing rates for each neuron in the hidden layer. 
    
    Want a row of entries showing the variance explained for each neuron based on regressors to order n
    And then two matrices showing the accuracy of each individual regressors for each neuron.
    
    And the a plot showing average activity conditioning 
    
    ''' 
    if data:
        episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
        episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
    else:
        episode_states, episode_actions, episode_rewards, episode_hiddens = generate_data(model, env, nits)
    
    
    actions = stack_order(episode_actions,order)
    rewards = stack_order(episode_rewards,order)
    combined = np.vstack([actions,rewards])
    
    # X = np.vstack([stack_order(episode_actions,order), stack_order(episode_rewards,order)])

    
    model_encoding_action = np.zeros((model.hidden_dim,order))
    model_encoding_reward = np.zeros((model.hidden_dim,order))
    reward_var = np.zeros((model.hidden_dim,order))
    action_var = np.zeros((model.hidden_dim,order))
    combined_var = np.zeros((model.hidden_dim,order))
    all_neurons = stack_hidden(episode_hiddens,order, model.hidden_dim)
    for neuron in range(model.hidden_dim):
        Y = all_neurons[neuron]
        # result = linear_regression(X,Y)
        # a_var, r_var, c_var = variance_explained(Y,actions,rewards,combined,order)
        a_var, r_var, c_var = variance_explained_hidden(Y,actions,rewards,combined,order)

        reward_var[neuron] = r_var
        action_var[neuron] = a_var
        combined_var[neuron] = c_var
        
        # fit = result.x
        # model_encoding_action[neuron] = fit[:order]
        # model_encoding_reward[neuron] = fit[order:]
        # reward_var[neuron] = variance_explained(X,Y[1],fit)[0]
        # reward_var[neuron] = variance_explained(X,Y[0],fit)[0] # NOT RIGHT
        
    vars = {'action':action_var, 'reward':reward_var, 'combined':combined_var}
    
    if data:
        return vars
    
    plot_encoding(vars, model.hidden_dim, order)
    
        
def plot_encoding(vars, num_neurons, order):
    fig, axs = create_figure_encoding(num_neurons, order)
    for key in vars.keys():
        axs[0].plot(np.mean(vars[key],axis=0), label = key)
        axs[0].set_title('Average Population Encoding Predictive Accuracy')
        axs[0].set_xlabel('order')
        axs[0].set_ylabel('$Partial R^2$')
        # axs[0].axhline(0.5,linestyle='dashed', alpha=0.5)
        axs[0].legend()
        # axs[0].set_xticks(np.arange(order)+1)
    
    for i,key in enumerate(['action','reward','combined']):
        # im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        axs[i+1].set_title(key + ' coding')
        axs[i+1].set_xlabel('order')
        axs[i+1].set_ylabel('neuron')
    
    fig.colorbar(im, cax=axs[-1])

    
def create_figure_encoding(order, num_neurons):
    """
    Creates a figure with subplots for each order of the regressor. 
    Width varies with order so we can keep a the visualizations clean
    """

    fig = plt.figure(constrained_layout=True, figsize=(.2*order, 4+.2*num_neurons))
    spec = gridspec.GridSpec(ncols=10, nrows=2, figure=fig)
    ax1 = fig.add_subplot(spec[0,:])
    ax2 = fig.add_subplot(spec[1,:3])
    ax3 = fig.add_subplot(spec[1,3:6])
    ax4 = fig.add_subplot(spec[1,6:9])
    cbar = fig.add_subplot(spec[1,9])
    
    
    axs = [ax1, ax2, ax3, ax4, cbar]
    return fig, axs

def combined_coding(model, env, order = 1, nits = 1, data=False, save = ''):
    
    if not data:
        data = (*generate_data(model, env, nits),0,0)
        
    accuracy,_ = plot_population_decoding(model, env, order, nits, data)
    vars = plot_population_encoding(model, env, order, nits, data)
    
    if model.policy.RL is not None:

        episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices = data
        episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
        
        PFCChoices = np.stack(PFCChoices).squeeze()
        BGChoices = np.stack(BGChoices).squeeze()

        data = (episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices)
        
        decoding,_ = BG_decode(model,data, order=order)
        encoding = BG_encode(model, data, order=order) 
        
        accuracy.update(decoding)
        vars.update(encoding)
     
    plot_combined(accuracy, vars, model.hidden_dim, order, RL = (model.policy.RL is not None), save = save)
    
def plot_combined(accuracy, vars, num_neurons, order,RL=False, save = ''):
    fig, axs = create_figure_combined(num_neurons, order, RL = RL)
    
    for key in accuracy.keys():
        axs[0].plot(accuracy[key], label = key)
        axs[0].set_title('Population Decoding Predictive Accuracy')
        axs[0].set_xlabel('order')
        axs[0].set_ylabel('accuracy')
        axs[0].axhline(0.5,linestyle='dashed', alpha=0.5)
        axs[0].legend()
            
    for key in vars.keys():
        axs[1].plot(np.mean(vars[key],axis=0), label = key)
        axs[1].set_title('Average Population Encoding Variance Explained')
        axs[1].set_xlabel('order')
        axs[1].set_ylabel('$Partial R^2$')
        # axs[0].axhline(0.5,linestyle='dashed', alpha=0.5)
        axs[1].legend()
        # axs[0].set_xticks(np.arange(order)+1)
    
    for i,key in enumerate(list(vars.keys())):
        # im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        im = axs[i+2].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        axs[i+2].set_title(key + ' coding')
        axs[i+2].set_xlabel('order')
        axs[i+2].set_ylabel('neuron')
    
    fig.colorbar(im, cax=axs[-1])
    if len(save) > 0:
        plt.savefig(save + '_PopulationCoding.png')
    # plt.tight_layout()
    
def create_figure_combined(order, num_neurons,RL=False):
    """
    Creates a figure with subplots for each order of the regressor. 
    Width varies with order so we can keep a the visualizations clean
    """

    fig = plt.figure(constrained_layout=True, figsize=(.3*order, 4+.3*num_neurons))
    spec = gridspec.GridSpec(ncols=10+3*RL, nrows=3, figure=fig)
    ax1 = fig.add_subplot(spec[0,:])
    ax2 = fig.add_subplot(spec[1,:])
    ax3 = fig.add_subplot(spec[2,:3])
    ax4 = fig.add_subplot(spec[2,3:6])
    ax5 = fig.add_subplot(spec[2,6:9])

    cbar = fig.add_subplot(spec[2,9+3*RL])
    
    if RL==True:
        ax6 = fig.add_subplot(spec[2,9:12])        
        axs = [ax1, ax2, ax3, ax4, ax5, ax6, cbar]

    else:
        axs = [ax1, ax2, ax3, ax4, ax5, cbar]

    return fig, axs


def BG_coding(model, data, order = 1):
    '''
    Computes how much the RL Q values / actions are encoded in the RNN. I.E. if the RNN formed a model of the RL
    '''
    episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices = data
    episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
    
    PFCChoices = np.stack(PFCChoices).squeeze()
    BGChoices = np.stack(BGChoices).squeeze()
    
    data = (episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices)
    
    decoding,_ = BG_decode(model,data, order=order)
    encoding = BG_encode(model, data, order=order)
    

def BG_decode(model, data,order = 1):

    episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices = data
        
    # targets = [episode_actions, episode_rewards]
    targets = [BGChoices]
    all_vars = {}
    all_accuracy = {}
    label = ['RL']
    for i,tar in enumerate(targets):
        all_vars[label[i]] = np.zeros((model.hidden_dim,order))
        all_accuracy[label[i]] = []
        for ord in range(order):
            Y = ravel_order(tar,ord)
            #Apply ReLU to hidden state to get firing rate:
            episode_hiddens = model.activation(episode_hiddens)
            X = ravel_hidden(episode_hiddens,ord, model.hidden_dim)
            fit = logistic_regression_no_bias(X,Y).x
           
            # 0 is left / unrewarded, 1 is right / rewarded
            accuracy = np.mean(np.round(logit(X,fit,0)) == Y)
            all_accuracy[label[i]].append(accuracy)
            # check variance explained by each neuron (column in X)
            vars = pseudo_variance_explained(X,Y,fit)    
            all_vars[label[i]][:,ord] = vars[0]        
            # all_vars[label[i]][:,ord] = vars[1]        
        
    return all_accuracy, all_vars
        
    
def BG_encode(model, data, order=1):

    episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices = data

    actions = stack_order(episode_actions,order)

    
    # X = np.vstack([stack_order(episode_actions,order), stack_order(episode_rewards,order)])

    
    action_var = np.zeros((model.hidden_dim,order))
    all_neurons = stack_hidden(episode_hiddens,order, model.hidden_dim)
    for neuron in range(model.hidden_dim):
        Y = all_neurons[neuron]
        a_var= variance_explained_BG(Y,actions,order)

        action_var[neuron] = a_var
        
    vars = {'RL':action_var}
    
    return vars


def variance_explained_BG(Y,BG_actions, order):
    delta_action_rsqs = []
    TSS = np.var(Y) * len(Y)
    def RSS_rec(X,fit,Y):
        return np.sum((Y - np.dot(X.T,fit))**2)
    delta_a = 0
    for i in range(order):
        action_fit = linear_regression(BG_actions[:i+1].T,Y)
        delta_a = np.maximum(np.nan_to_num(1 - RSS_rec(BG_actions[:i+1],action_fit[0],Y) / TSS - sum(delta_action_rsqs), 0),0)
        delta_action_rsqs.append(delta_a)
    
    return delta_action_rsqs

def create_figure_BG(order, num_neurons):
    """
    Creates a figure with subplots for each order of the regressor. 
    Width varies with order so we can keep a the visualizations clean
    """

    fig = plt.figure(constrained_layout=True, figsize=(.3*order, 4+.3*num_neurons))
    spec = gridspec.GridSpec(ncols=10, nrows=3, figure=fig)
    ax1 = fig.add_subplot(spec[0,:])
    ax2 = fig.add_subplot(spec[1,:])
    ax3 = fig.add_subplot(spec[2,:3])
    ax4 = fig.add_subplot(spec[2,3:6])
    ax5 = fig.add_subplot(spec[2,6:9])
    cbar = fig.add_subplot(spec[2,9])
    
    
    axs = [ax1, ax2, ax3, ax4, ax5, cbar]
    # for i in len(axs):
    #     axs[i].
    return fig, axs


def model_coding_comparison(model_list, model_labels, env_list, nits = 1000, order =10, save = ''):
    from analysis_scripts.test_suite import generate_data as generate_data_comparison

    fig, axs = plt.subplots(nrows=3, ncols=1,figsize = (12,9), dpi=200)
    ax_labels = ['Action Decoding', 'Reward Decoding', 'RL Action Decoding']
    fig.suptitle('Variable Decoding')
    
    fig2, axs2 = plt.subplots(nrows=3, ncols=1, figsize=(12,9), dpi=200)
    ax2_labels = ['Action Encoding', 'Reward Encoding', 'RL Action Encoding']
    fig2.suptitle('Variable Encoding')
    
    keys = ['action', 'reward', 'RL']
    colors = ['C0', 'C1', 'C2', 'C3']
    
    for model, label, env, c in zip(model_list,model_labels, env_list, colors):
        data  = generate_data_comparison(model, env, nits=nits)      
        # episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
        decode_accuracy, _ = plot_population_decoding(model, env, order= order, nits = nits, data = data)
        encode_vars = plot_population_encoding(model, env, order=order, data=data)
        
        if model.policy.RL is not None:
            episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices = data
            episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)
            
            PFCChoices = np.stack(PFCChoices).squeeze()
            BGChoices = np.stack(BGChoices).squeeze()

            data = (episode_states, episode_actions, episode_rewards, episode_hiddens, PFCChoices, BGChoices)
            
            decoding,_ = BG_decode(model,data, order=order)
            encoding = BG_encode(model, data, order=order) 
            
            decode_accuracy.update(decoding)
            encode_vars.update(encoding)
            
        for i in range(len(axs)):
            try:
                axs[i].plot(decode_accuracy[keys[i]], label = label, color = c)
                axs[i].set_title(ax_labels[i])
                axs[i].set_ylabel('Predictive Accuracy')
                fig.legend(model_labels)
                
                axs2[i].plot(np.mean(encode_vars[keys[i]], axis=0), label = label, color = c)
                axs2[i].set_title(ax2_labels[i])
                axs2[i].set_xlabel('Order (trials back)')
                axs2[i].set_ylabel('Partial R^2 per Neuron')        
            except:
                pass
        fig.legend(model_labels)
        fig2.legend(model_labels)
        
        if save != '':
            fig.save(save + '_Decoding.png')
            fig2.save(save+'_Encoding.png')
        plt.tight_layout()
        fig.show()
        fig2.show()


def variance_explained_hidden_paper(Y,Y_reduced,actions, rewards, win_stay,lose_switch,win_switch,lose_stay,order):
    '''
    given a fit, returns the variance explained by the model as well as by each individual regressor 
    '''

    delta_action_rsqs = []
    delta_reward_rsqs = []
    delta_ws_rsqs = []
    delta_ls_rsqs = []
    delta_wsw_rsqs = []
    delta_lst_rsqs = []
    # Y_rec = np.maximum(0,Y)
    TSS = np.var(Y) * len(Y)
    TSS_r = np.var(Y_reduced) * len(Y_reduced)
    
    def RSS_rec(X,fit,Y):
        return np.sum((Y - np.dot(X.T,fit))**2)
    
    delta_a = delta_r = delta_ws = delta_ls = delta_wsw = delta_lst = 0
    
    for i in range(order):

    #     action_fit = linear_regression(actions[:i+1].T,Y)
    #     reward_fit = linear_regression(rewards[:i+1].T,Y)
    #     combined_fit = linear_regression(combined[:i+1].T,Y)
    #     delta_a = np.nan_to_num(1 - RSS(Y,yhat(actions[:i+1],action_fit)) / TSS - sum(delta_action_rsqs) ,0)
    #     delta_r = np.nan_to_num(1 - RSS(Y,yhat(rewards[:i+1],reward_fit)) / TSS - sum(delta_reward_rsqs), 0) 
    #     delta_c = np.nan_to_num(1 - RSS(Y,yhat(combined[:i+1],combined_fit)) / TSS - sum(delta_combined_rsqs), 0)
        
        action_fit = linear_regression(actions[:i+1].T,Y)
        reward_fit = linear_regression(rewards[:i+1].T,Y)
        win_stay_fit = linear_regression(win_stay[:i+1].T,Y_reduced)
        lose_switch_fit = linear_regression(lose_switch[:i+1].T,Y_reduced )
        win_switch_fit = linear_regression(win_switch[:i+1].T,Y_reduced)
        lose_stay_fit = linear_regression(lose_stay[:i+1].T,Y_reduced)
        
        
        delta_a = np.maximum(np.nan_to_num(1 - RSS_rec(actions[:i+1],action_fit[0],Y) / TSS - sum(delta_action_rsqs), 0),0)
        delta_r = np.maximum(np.nan_to_num(1 - RSS_rec(rewards[:i+1],reward_fit[0],Y) / TSS - sum(delta_reward_rsqs), 0),0)
        delta_ws = np.maximum(np.nan_to_num(1 - RSS_rec(win_stay[:i+1],win_stay_fit[0],Y_reduced) / TSS_r - sum(delta_ws_rsqs), 0),0)
        delta_ls = np.maximum(np.nan_to_num(1 - RSS_rec(lose_switch[:i+1],lose_switch_fit[0],Y_reduced) / TSS_r - sum(delta_ls_rsqs), 0),0)        
        delta_wsw = np.maximum(np.nan_to_num(1 - RSS_rec(win_switch[:i+1],win_switch_fit[0],Y_reduced) / TSS_r - sum(delta_wsw_rsqs), 0),0)
        delta_lst = np.maximum(np.nan_to_num(1 - RSS_rec(lose_stay[:i+1],lose_stay_fit[0],Y_reduced) / TSS_r - sum(delta_lst_rsqs), 0),0)
        # delta_a = np.fmax(0,np.nan_to_num(1 - action_fit[1] / TSS - sum(delta_action_rsqs), 0))
        # delta_r = np.fmax(0,np.nan_to_num(1 - reward_fit[1] / TSS - sum(delta_reward_rsqs), 0))
        # delta_c = np.fmax(0,np.nan_to_num(1 - combined_fit[1] / TSS - sum(delta_combined_rsqs), 0))
            
        delta_action_rsqs.append(delta_a)
        delta_reward_rsqs.append(delta_r)
        delta_ws_rsqs.append(delta_ws) 
        delta_ls_rsqs.append(delta_ls)
        delta_wsw_rsqs.append(delta_wsw)
        delta_lst_rsqs.append(delta_lst)    
    return delta_action_rsqs, delta_reward_rsqs, delta_ws_rsqs, delta_ls_rsqs, delta_wsw_rsqs, delta_lst_rsqs

# fits a linear regression model to the data one term at a time
# def sequential_regression(X,Y):
#     fits = []
#     for i in range(X.shape[0]):
#         fit = linear_regression_l2(X[np.newaxis,i],Y).x
#         fits.append(fit)
#         Y = Y - X[i]*fit
        
#     return np.array(fits)

def sequential_regression(X,Y):
    fits = []
    # first subtract the mean since we only care about the encoding
    Y = Y - np.mean(Y)
    
    for i in range(X.shape[0]):
        fit = linear_regression_l2(X[np.newaxis,i],Y)
        # fit_err = np.sqrt(fit.jac[0])
        sigsquared = np.sum((Y - X[i]*fit.x)**2) / (len(Y) - 1)
        fit_err = fit.hess_inv[0]# what is RSS
        fit_err = 1.96 * np.sqrt(fit_err * sigsquared)
        fit = fit.x
        Y = Y - X[i]*fit
        
        # if the weight is too close to zero, set to 0 because we want to filter out noise
        
        if fit > 0:
            if fit - fit_err < 0:
                fits.append(np.array([0]))
            else:
                fits.append(fit)
                
        else:
            if fit + fit_err > 0:
                fits.append(np.array([0]))
            else:
                fits.append(fit)

    return np.array(fits)
    

def weights_hidden_paper(Y,Y_reduced,actions, rewards, win_stay,lose_switch,win_switch,lose_stay,order):
    '''
    given a fit, returns the variance explained by the model as well as by each individual regressor 
    '''

    delta_action_rsqs = []
    delta_reward_rsqs = []
    delta_ws_rsqs = []
    delta_ls_rsqs = []
    delta_wsw_rsqs = []
    delta_lst_rsqs = []
    # Y_rec = np.maximum(0,Y)
    TSS = np.var(Y) * len(Y)
    TSS_r = np.var(Y_reduced) * len(Y_reduced)
    
    def RSS_rec(X,fit,Y):
        return np.sum((Y - np.dot(X.T,fit))**2)
    
    delta_a = delta_r = delta_ws = delta_ls = delta_wsw = delta_lst = 0
    
    # for i in range(order):
    Y = Y - np.mean(Y)
    Y_reduced = Y_reduced - np.mean(Y_reduced)
    Y = Y / np.std(Y)
    Y_reduced = Y_reduced / np.std(Y_reduced)

    action_fit = sequential_regression(actions[:order+1],Y)
    reward_fit = sequential_regression(rewards[:order+1],Y)
    win_stay_fit = sequential_regression(win_stay[:order+1],Y_reduced)
    lose_switch_fit = sequential_regression(lose_switch[:order+1],Y_reduced )
    win_switch_fit = sequential_regression(win_switch[:order+1],Y_reduced)
    lose_stay_fit = sequential_regression(lose_stay[:order+1],Y_reduced)
    
        # delta_a = np.maximum(np.nan_to_num(1 - RSS_rec(actions[:i+1],action_fit[0],Y) / TSS - sum(delta_action_rsqs), 0),0)
        # delta_r = np.maximum(np.nan_to_num(1 - RSS_rec(rewards[:i+1],reward_fit[0],Y) / TSS - sum(delta_reward_rsqs), 0),0)
        # delta_ws = np.maximum(np.nan_to_num(1 - RSS_rec(win_stay[:i+1],win_stay_fit[0],Y_reduced) / TSS_r - sum(delta_ws_rsqs), 0),0)
        # delta_ls = np.maximum(np.nan_to_num(1 - RSS_rec(lose_switch[:i+1],lose_switch_fit[0],Y_reduced) / TSS_r - sum(delta_ls_rsqs), 0),0)        
        
        # delta_a = np.fmax(0,np.nan_to_num(1 - action_fit[1] / TSS - sum(delta_action_rsqs), 0))
        # delta_r = np.fmax(0,np.nan_to_num(1 - reward_fit[1] / TSS - sum(delta_reward_rsqs), 0))
        # delta_c = np.fmax(0,np.nan_to_num(1 - combined_fit[1] / TSS - sum(delta_combined_rsqs), 0))
            
        # delta_action_rsqs.append(delta_a)
        # delta_reward_rsqs.append(delta_r)
        # delta_ws_rsqs.append(delta_ws) 
        # delta_ls_rsqs.append(delta_ls)
    
    return action_fit, reward_fit, win_stay_fit, lose_switch_fit, win_switch_fit, lose_stay_fit


def plot_encoding_paper(var, weights, num_neurons, order,axs, **plot_kwargs):

    for i,key in enumerate(['win_stay','lose_switch']):
        # im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
        im = axs[i].matshow(weights[key], aspect='auto', vmin=-1, vmax=1, cmap='bwr')
        axs[i].set_title(key.replace('_',' ') + ' coding', fontsize=16)
        axs[i].set_xlabel('order', fontsize=12)
        axs[i].set_ylabel('neuron', fontsize=12)
    

# def plot_encoding_paper(var, weights, num_neurons, order,axs, **plot_kwargs):
    
#     not_action = 0
#     for key in var.keys():
#         x = np.arange(not_action,len(var[key][0]))
#         axs[0].plot(x,np.mean(var[key],axis=0)[not_action:], label = key,**plot_kwargs)
#         axs[0].set_title('Average Population Encoding Predictive Accuracy')
#         axs[0].set_xlabel('order')
#         axs[0].set_ylabel('$Partial R^2$')
#         if not_action == 0:
#             not_action = 1
#         # axs[0].axhline(0.5,linestyle='dashed', alpha=0.5)
#     axs[0].legend(list(var.keys()))
#         # axs[0].set_xticks(np.arange(order)+1)
    
#     for i,key in enumerate(['win_stay','lose_switch']):
#         # im = axs[i+1].matshow(vars[key], aspect='auto', vmin=0, vmax=1, cmap='plasma')
#         im = axs[i+1].matshow(weights[key], aspect='auto', vmin=-1, vmax=1, cmap='bwr')
#         axs[i+1].set_title(key.replace('_',' ') + ' coding')
#         axs[i+1].set_xlabel('order')
#         axs[i+1].set_ylabel('neuron')
    
    
    # fig.colorbar(im, cax=axs[-1])


def plot_population_encoding_paper(model,data, order = 1, nits = 2,  ax = None, multilayer = True,timescale=True, **plot_kwargs):
    '''
    Uses behavioral data to try to predict individual firing rates for each neuron in the hidden layer. 
    
    Want a row of entries showing the variance explained for each neuron based on regressors to order n
    And then two matrices showing the accuracy of each individual regressors for each neuron.
    
    And the a plot showing average activity conditioning 
    
    ''' 
    
    raw_weights = {}
    episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
    episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)

    rw = []
    cl = []
    cw = []
    rl = []
    for i in range(len(episode_actions)):
        temp_rw = []
        temp_cl = []
        temp_cw = []
        temp_rl = []
        for j in range(1,len(episode_actions[i])):
            lagrew = episode_rewards[i][j-1]
            is_stay = episode_actions[i][j] == episode_actions[i][j-1]
            is_win = lagrew == 1
            
            rw_val, cl_val, cw_val, rl_val = 0, 0, 0, 0

            action_val = 1 if episode_actions[i][j] == 1 else -1
            
            if is_win and is_stay: # Win-Stay
                rw_val = action_val
            elif is_win and not is_stay: # Win-Switch
                cw_val = action_val
            elif not is_win and not is_stay: # Lose-Switch
                cl_val = action_val
            elif not is_win and is_stay: # Lose-Stay
                rl_val = action_val

            temp_rw.append(rw_val)
            temp_cl.append(cl_val)
            temp_cw.append(cw_val)
            temp_rl.append(rl_val)
        rw.append(temp_rw)
        cl.append(temp_cl) 
        cw.append(temp_cw)
        rl.append(temp_rl)
        
    
    actions = stack_order(episode_actions,order)
    rewards = stack_order(episode_rewards,order)
    rw = stack_order(torch.tensor(rw), order)
    cl = stack_order(torch.tensor(cl), order)
    cw = stack_order(torch.tensor(cw), order)
    rl = stack_order(torch.tensor(rl), order)
    
    # X = np.vstack([stack_order(episode_actions,order), stack_order(episode_rewards,order)])

    
  
    reward_var = np.zeros((model.hidden_dim,order))
    action_var = np.zeros((model.hidden_dim,order))
    win_stay_var = np.zeros((model.hidden_dim,order))
    lose_switch_var = np.zeros((model.hidden_dim,order))
    win_switch_var = np.zeros((model.hidden_dim,order))
    lose_stay_var = np.zeros((model.hidden_dim,order))
    
    reward_w = np.zeros((model.hidden_dim,order))
    action_w = np.zeros((model.hidden_dim,order))
    win_stay_w = np.zeros((model.hidden_dim,order))
    lose_switch_w = np.zeros((model.hidden_dim,order))
    win_switch_w = np.zeros((model.hidden_dim,order))
    lose_stay_w = np.zeros((model.hidden_dim,order))
    
    # if multilayer, we need to feed the hidden layer through the linear layers
    if multilayer: # layer = 1 input, layer = 2 processing, layer = 3 output
        l2 = model.policy.linear2
        l3 = model.policy.linear3
        episode_hiddens = [l2(hidden.T).T for hidden in episode_hiddens]
        # try with and without activation
        # first without activation
        episode_hiddens = torch.stack(episode_hiddens, dim = 0)
        # episode_hiddens = model.policy.activation(episode_hiddens)
        # episode_hiddens = torch.matmul(l3.T,torch.ones_like(l3)).T (or maybe it's no)
        
    
    all_neurons = stack_hidden(episode_hiddens,order, model.hidden_dim)
    all_neurons_reduced = stack_hidden(episode_hiddens,order+1, model.hidden_dim)
    timescale_dict = {'action':[], 'reward':[], 'win_stay':[], 'win_switch':[], 'lose_stay':[], 'lose_switch':[]}
    signs_dict = {'action':[], 'reward':[], 'win_stay':[], 'win_switch':[], 'lose_stay':[], 'lose_switch':[]} # counting sign reversals per neuron
    for neuron in range(model.hidden_dim):
        Y = all_neurons[neuron]
        Y_reduced = all_neurons_reduced[neuron]
        # result = linear_regression(X,Y)
        # a_var, r_var, c_var = variance_explained(Y,actions,rewards,combined,order)
        a_w, r_w, rw_w, cl_w, cw_w, rl_w = weights_hidden_paper(Y, Y_reduced,actions,rewards,rw, cl,cw,rl,order)
        if timescale:
            ta,tr,tws,tls,twsw,tlsst = neuronal_timescales(a_w, r_w, rw_w, cl_w, cw_w, rl_w)
            timescale_dict['action'].append(ta)
            timescale_dict['reward'].append(tr)
            timescale_dict['win_stay'].append(tws)
            timescale_dict['win_switch'].append(tls)
            timescale_dict['lose_stay'].append(twsw)
            timescale_dict['lose_switch'].append(tlsst)
        
            sa, sr, sws, sls, swsw, slst = sign_reversals(a_w, r_w, rw_w, cl_w, cw_w, rl_w)
            signs_dict['action'].append(sa)
            signs_dict['reward'].append(sr)
            signs_dict['win_stay'].append(sws)
            signs_dict['win_switch'].append(sls)
            signs_dict['lose_stay'].append(swsw)
            signs_dict['lose_switch'].append(slst)
        
        
        
        reward_w[neuron] = r_w.squeeze()
        action_w[neuron] = a_w.squeeze()
        win_stay_w[neuron] = rw_w.squeeze()
        lose_switch_w[neuron] = cl_w.squeeze()
        win_switch_w[neuron] = cw_w.squeeze()
        lose_stay_w[neuron] = rl_w.squeeze()
        
        a_var, r_var, rw_var, cl_var, cw_var, rl_var = variance_explained_hidden_paper(Y, Y_reduced,actions,rewards,rw, cl,cw,rl,order)
        

        reward_var[neuron] = r_var
        action_var[neuron] = a_var
        win_stay_var[neuron] = rw_var
        lose_switch_var[neuron] = cl_var
        win_switch_var[neuron] = cw_var
        lose_stay_var[neuron] = rl_var
        
        
        
        # fit = result.x
        # model_encoding_action[neuron] = fit[:order]
        # model_encoding_reward[neuron] = fit[order:]
        # reward_var[neuron] = variance_explained(X,Y[1],fit)[0]
        # reward_var[neuron] = variance_explained(X,Y[0],fit)[0] # NOT RIGHT
        
    var = {'action':action_var,  'win_stay':win_stay_var, 'lose_switch':lose_switch_var, 'reward':reward_var, 'win_switch':win_switch_var, 'lose_stay':lose_stay_var }
    weights = {'action':action_w,  'win_stay':win_stay_w, 'lose_switch':lose_switch_w, 'reward':reward_w, 'win_switch':win_switch_w, 'lose_stay':lose_stay_w }
    # if data:
    #     return var
    
    if ax == None:
        # plot_encoding(var, model.hidden_dim, order)
        pass
    else:
        # ax should be an iterable of the form [average encoding axis, ws individuals axis, lsxindividuals axis]
        plot_encoding_paper(var,weights, model.hidden_dim, order, ax, **plot_kwargs)
    return timescale_dict, signs_dict, weights, var



# def plot_population_encoding_paper(model,data, order = 1, nits = 2,  ax = None, **plot_kwargs):
#     '''
#     Uses behavioral data to try to predict individual firing rates for each neuron in the hidden layer. 
    
#     Want a row of entries showing the variance explained for each neuron based on regressors to order n
#     And then two matrices showing the accuracy of each individual regressors for each neuron.
    
#     And the a plot showing average activity conditioning 
    
#     ''' 
#     episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
#     episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)

#     ws = []
#     ls = []
#     for i in range(len(episode_actions)):
#         temp_ws = []
#         temp_ls = []
#         for j in range(1,len(episode_actions[i])):
#             lagrew = episode_rewards[i][j-1]
#             action = episode_actions[i][j] == episode_actions[i][j-1]
#             if lagrew == 1: 
#                 temp_ls.append(0)
#                 if action == 1:
#                     if episode_actions[i][j] == 1:
#                         temp_ws.append(1)
#                     else:
#                         temp_ws.append(-1)
#                 else:
#                     temp_ws.append(0)
#             else:
#                 temp_ws.append(0)
#                 if action == 0:
#                     if episode_actions[i][j] == 1:
#                         temp_ls.append(1)
#                     else:
#                         temp_ls.append(-1)
#                 else:
#                     temp_ls.append(0)
#         ws.append(temp_ws)
#         ls.append(temp_ls) 
    
#     actions = stack_order(episode_actions,order)
#     rewards = stack_order(episode_rewards,order)
#     ws = stack_order(torch.tensor(ws), order)
#     ls = stack_order(torch.tensor(ls), order)
    
#     # X = np.vstack([stack_order(episode_actions,order), stack_order(episode_rewards,order)])

    
  
#     reward_var = np.zeros((model.hidden_dim,order))
#     action_var = np.zeros((model.hidden_dim,order))
#     win_stay_var = np.zeros((model.hidden_dim,order))
#     lose_switch_var = np.zeros((model.hidden_dim,order))
    
#     reward_w = np.zeros((model.hidden_dim,order))
#     action_w = np.zeros((model.hidden_dim,order))
#     win_stay_w = np.zeros((model.hidden_dim,order))
#     lose_switch_w = np.zeros((model.hidden_dim,order))
    
#     all_neurons = stack_hidden(episode_hiddens,order, model.hidden_dim)
#     all_neurons_reduced = stack_hidden(episode_hiddens,order+1, model.hidden_dim)
#     timescale_dict = {'action':[], 'reward':[], 'win_stay':[], 'lose_switch':[]}
#     signs_dict = {'action':[], 'reward':[], 'win_stay':[], 'lose_switch':[]} # counting sign reversals per neuron
#     for neuron in range(model.hidden_dim):
#         Y = all_neurons[neuron]
#         Y_reduced = all_neurons_reduced[neuron]
#         # result = linear_regression(X,Y)
#         # a_var, r_var, c_var = variance_explained(Y,actions,rewards,combined,order)
#         a_w, r_w, ws_w, ls_w = weights_hidden_paper(Y, Y_reduced,actions,rewards,ws, ls,order)

#         ta,tr,tws,tls = neuronal_timescales(a_w, r_w, ws_w, ls_w)
#         timescale_dict['action'].append(ta)
#         timescale_dict['reward'].append(tr)
#         timescale_dict['win_stay'].append(tws)
#         timescale_dict['lose_switch'].append(tls)
        
#         sa, sr, sws, sls = sign_reversals(a_w, r_w, ws_w, ls_w)
#         signs_dict['action'].append(sa)
#         signs_dict['reward'].append(sr)
#         signs_dict['win_stay'].append(sws)
#         signs_dict['lose_switch'].append(sls)
        
        
        
#         reward_w[neuron] = r_w.squeeze()
#         action_w[neuron] = a_w.squeeze()
#         win_stay_w[neuron] = ws_w.squeeze()
#         lose_switch_w[neuron] = ls_w.squeeze()
        
#         a_var, r_var, ws_var, ls_var = variance_explained_hidden_paper(Y, Y_reduced,actions,rewards,ws, ls,order)
        

#         reward_var[neuron] = r_var
#         action_var[neuron] = a_var
#         win_stay_var[neuron] = ws_var
#         lose_switch_var[neuron] = ls_var
        
        

        
#         # fit = result.x
#         # model_encoding_action[neuron] = fit[:order]
#         # model_encoding_reward[neuron] = fit[order:]
#         # reward_var[neuron] = variance_explained(X,Y[1],fit)[0]
#         # reward_var[neuron] = variance_explained(X,Y[0],fit)[0] # NOT RIGHT
        
#     var = {'action':action_var,  'win_stay':win_stay_var, 'lose_switch':lose_switch_var, 'reward':reward_var }
#     weights = {'action':action_w,  'win_stay':win_stay_w, 'lose_switch':lose_switch_w, 'reward':reward_w }
#     # if data:
#     #     return var
    
#     if ax == None:
#         # plot_encoding(var, model.hidden_dim, order)
#         pass
#     else:
#         # ax should be an iterable of the form [average encoding axis, ws individuals axis, lsxindividuals axis]
#         plot_encoding_paper(var,weights, model.hidden_dim, order, ax, **plot_kwargs)
#     return timescale_dict, signs_dict


def plot_code_switching_paper(data, order = 5, ax = None):
    episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
    episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)

    neurons = episode_hiddens.detach().numpy() #neuron view
    
    
    ws = []
    ls = []
    for i in range(len(episode_actions)):
        temp_ws = []
        temp_ls = []
        for j in range(1,len(episode_actions[i])):
            lagrew = episode_rewards[i][j-1]
            action = episode_actions[i][j] == episode_actions[i][j-1]
            if lagrew == 1: 
                temp_ls.append(0)
                if action == 1:
                    if episode_actions[i][j] == 1:
                        temp_ws.append(1)
                    else:
                        temp_ws.append(-1)
                else:
                    temp_ws.append(0)
            else:
                temp_ws.append(0)
                if action == 0:
                    if episode_actions[i][j] == 1:
                        temp_ls.append(1)
                    else:
                        temp_ls.append(-1)
                else:
                    temp_ls.append(0)
        ws.append(temp_ws)
        ls.append(temp_ls) 
    
    # ws = stack_order(torch.tensor(ws), order+1)
    # ls = stack_order(torch.tensor(ls), order+1)        
    ws_coeffs = process_code_switching(neurons, ws, order = order)
    ls_coeffs = process_code_switching(neurons, ls, order = order)
    
    ws_counts = [1]
    ls_counts = [1]

    for i in range(1,order+1):
        # ws_counts.append(np.sum(ws_coeffs[:,i] >= 0))
        # ls_counts.append(np.sum(ls_coeffs[:,i] >= 0)) 
        ws_counts.append(np.mean(np.sign(ws_coeffs[:,i]) ==np.sign(ws_coeffs[:,i-1])))
        ls_counts.append(np.mean(np.sign(ls_coeffs[:,i]) ==np.sign(ls_coeffs[:,i-1])))
    plot_switching(ws_counts, ls_counts, ax)
    
def plot_switching(ws_counts, ls_counts, ax):
    if ax == None:
        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,9), dpi=200)
    ax.plot(ws_counts, label = 'WS')
    ax.plot(ls_counts, label = 'LS')
    ax.set_title('Code Switching')
    ax.set_xlabel('order')
    ax.set_ylabel('+ encoding')
    ax.legend()
    
    
def process_code_switching(neurons, x, order = 5):
    '''
    takes the neuronal data and the x data and processes it to look for code sign switching in the hidden layer
    fits a N*ord regressions to each x to predict neuronal activity for order ord
    ''' 
    x_ravels = []
    for i in range(order,-1,-1):
        if i == order:
            x_ravel = np.hstack([x[j][i:] for j in range(len(x))])
        else:
            x_ravel = np.hstack([x[j][i:-order+i] for j in range(len(x))])
        x_ravels.append(x_ravel)     
    
    coeffs = np.zeros((neurons.shape[1],order+1))
    # for batch in range(neurons.shape[0]):
    #     y_ravel = np.hstack([neurons[j][:,batch][j] for j in range(len(x))])
    #     for i in range(order+1):
    #         coeffs[batch,i] = linear_regression_bias(x_ravels[i],y_ravel)[0]
    neurons = neurons[:,:,order+1:]
    for neuron in range(neurons.shape[0]):        
        y_ravel = np.hstack([neurons[j,neuron] for j in range(neurons.shape[0])])
        for i in range(order+1):
            coeffs[neuron,i] = linear_regression(x_ravels[i][:,np.newaxis],y_ravel)[0][0] # don't care about bias, just the sign of the coefficient            
            # coeffs[neuron,i] = linear_regression_bias(x_ravels[i],y_ravel)[0][0] # don't care about bias, just the sign of the coefficient
    return coeffs






def plot_code_switching_paper(data, order = 5, ax = None):
    episode_states, episode_actions, episode_rewards, episode_hiddens, _, _ = data
    episode_states, episode_actions, episode_rewards, episode_hiddens = format_data(episode_states, episode_actions, episode_rewards, episode_hiddens)

    neurons = episode_hiddens.detach().numpy() #neuron view
    
    
    ws = []
    ls = []
    for i in range(len(episode_actions)):
        temp_ws = []
        temp_ls = []
        for j in range(1,len(episode_actions[i])):
            lagrew = episode_rewards[i][j-1]
            action = episode_actions[i][j] == episode_actions[i][j-1]
            if lagrew == 1: 
                temp_ls.append(0)
                if action == 1:
                    if episode_actions[i][j] == 1:
                        temp_ws.append(1)
                    else:
                        temp_ws.append(-1)
                else:
                    temp_ws.append(0)
            else:
                temp_ws.append(0)
                if action == 0:
                    if episode_actions[i][j] == 1:
                        temp_ls.append(1)
                    else:
                        temp_ls.append(-1)
                else:
                    temp_ls.append(0)
        ws.append(temp_ws)
        ls.append(temp_ls) 
    
    # ws = stack_order(torch.tensor(ws), order+1)
    # ls = stack_order(torch.tensor(ls), order+1)        
    ws_coeffs = process_code_switching(neurons, ws, order = order)
    ls_coeffs = process_code_switching(neurons, ls, order = order)
    
    ws_counts = [1]
    ls_counts = [1]

    for i in range(1,order+1):
        # ws_counts.append(np.sum(ws_coeffs[:,i] >= 0))
        # ls_counts.append(np.sum(ls_coeffs[:,i] >= 0)) 
        ws_counts.append(np.mean(np.sign(ws_coeffs[:,i]) ==np.sign(ws_coeffs[:,i-1])))
        ls_counts.append(np.mean(np.sign(ls_coeffs[:,i]) ==np.sign(ls_coeffs[:,i-1])))
    plot_switching(ws_counts, ls_counts, ax)
    
    
def neuronal_timescales(action_var, reward_var, ws_var, ls_var, wsw_var, lst_var):
    '''
    computes the distribution of action, reward, ws, and ls timescales for each neuron 
    does this by fitting an exponential decay to each set of coefficients
    '''
    
    expf = lambda x, a, b: a*np.exp(-x/b)
    
    def fit_timescale(var):
        # fit 10 times with different initial conditions, keep best
        best = []
        var = np.abs(var)
        res = minimize(lambda x: np.sum((expf(np.arange(len(var)),*x) - var.T)**2), [1,1])
        for i in range(9):
            res2 = minimize(lambda x: np.sum((expf(np.arange(len(var)),*x) - var.T)**2), np.array([np.random.rand(1)*2-1,10*np.random.rand(1)]).squeeze())
            if res2.fun < res.fun:
                res = res2
            # compute Bic
            bic_res = np.sum((expf(np.arange(len(var)),*res.x) - var.T)**2)
            # compare to bic
                            
        return res.x[1]

    # want to compare this to just fitting a constant line, i.e. no timescale
    # maybe 
    
    
    ta = fit_timescale(action_var)
    tr = fit_timescale(reward_var)
    tws = fit_timescale(ws_var)
    tls = fit_timescale(ls_var)
    twsw = fit_timescale(wsw_var)
    tlst = fit_timescale(lst_var)
    
    return (ta, tr, tws, tls, twsw, tlst)

def sign_reversals(action_var, reward_var, ws_var, ls_var, wsw_var, lst_var):
    
    def count_sign_reversals(var):
        # return np.sum(np.diff(np.sign(var)) != 0)
        return np.sum(np.diff(np.sign(var) !=0 ,axis=0) )
    
    # def count_sign_reversals(var): # only counts changes in the first sign    
    #     return np.sum(np.sign(var[:,0]) != np.sign(var[:,1]))
    
    sa = count_sign_reversals(action_var)
    sr = count_sign_reversals(reward_var)
    sws = count_sign_reversals(ws_var)
    sls = count_sign_reversals(ls_var)
    swsw = count_sign_reversals(wsw_var)
    slst = count_sign_reversals(lst_var)
    
    return (sa, sr, sws, sls, swsw, slst)
# def neuronal_timescales(action_var, reward_var, ws_var, ls_var):
#     '''
#     computes the distribution of action, reward, ws, and ls timescales for each neuron 
#     does this by fitting an exponential decay, double exponential decay, and a constant to each set of coefficients
#     '''
    
#     expf = lambda x, a, b: a * np.exp(-b * x)
#     double_expf = lambda x, a, b, c, d: a * np.exp(-b * x) + c * np.exp(-d * x)
#     constf = lambda x, a: a
    
#     def fit_timescale(var):
#         x_data = np.arange(len(var))
        
#         # Fit single exponential
#         res_exp = minimize(lambda x: np.sum((expf(x_data, *x) - var)**2), [1, 1])
#         timescale_exp = 1 / res_exp.x[1]
        
#         # Fit double exponential
#         res_double_exp = minimize(lambda x: np.sum((double_expf(x_data, *x) - var)**2), [1, 1, 1, 1])
#         timescale_double_exp = 1 / res_double_exp.x[1]  # Using the first timescale component
        
#         # Fit constant
#         res_const = minimize(lambda x: np.sum((constf(x_data, *x) - var)**2), [1])
#         timescale_const = float('inf')  # Constant has no timescale
        
#         # Determine the best fit based on the residual sum of squares
#         rss_exp = np.sum((expf(x_data, *res_exp.x) - var)**2)
#         rss_double_exp = np.sum((double_expf(x_data, *res_double_exp.x) - var)**2)
#         rss_const = np.sum((constf(x_data, *res_const.x) - var)**2)
        
#         best_fit = min(rss_exp, rss_double_exp, rss_const)
        
#         if best_fit == rss_exp:
#             return timescale_exp
#         elif best_fit == rss_double_exp:
#             return timescale_double_exp
#         else:
#             return timescale_const
    
#     ta = fit_timescale(action_var)
#     tr = fit_timescale(reward_var)
#     tws = fit_timescale(ws_var)
#     tls = fit_timescale(ls_var)
    
#     return (ta, tr, tws, tls)